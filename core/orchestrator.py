"""
QuantumForge V4 Intelligent Tool Orchestrator

LLMé©±åŠ¨çš„å·¥å…·ç¼–æ’å™¨ï¼Œè´Ÿè´£åŠ¨æ€é€‰æ‹©å·¥å…·å’Œæ§åˆ¶æ‰§è¡Œæµç¨‹ã€‚
"""

from typing import Dict, Any, Optional, List
from memory import Memory
from llm_engine import call_llm
import json
import time
from datetime import datetime


class ResourceManager:
    """
    èµ„æºç®¡ç†å™¨ - æ§åˆ¶LLMè°ƒç”¨ã€tokenä½¿ç”¨å’Œæ‰§è¡Œæ—¶é—´
    """
    
    def __init__(self):
        # èµ„æºé™åˆ¶é…ç½®
        self.max_llm_calls = 20            # æœ€å¤§LLMè°ƒç”¨æ¬¡æ•°
        self.max_total_tokens = 100000     # æœ€å¤§æ€»tokenæ•°
        self.max_execution_time = 300      # æœ€å¤§æ‰§è¡Œæ—¶é—´(ç§’)
        self.max_context_length = 15000    # æœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦
        
        # å½“å‰ä½¿ç”¨ç»Ÿè®¡
        self.current_llm_calls = 0
        self.current_tokens = 0
        self.start_time = None
        self.execution_log = []
        
    def start_execution(self):
        """å¼€å§‹æ‰§è¡Œï¼Œé‡ç½®ç»Ÿè®¡"""
        self.start_time = time.time()
        self.current_llm_calls = 0
        self.current_tokens = 0
        self.execution_log = []
        
    def can_make_llm_call(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å¯ä»¥è¿›è¡ŒLLMè°ƒç”¨"""
        if self.current_llm_calls >= self.max_llm_calls:
            return False
        
        if self.start_time and (time.time() - self.start_time) > self.max_execution_time:
            return False
            
        return True
    
    def record_llm_call(self, prompt_tokens: int, response_tokens: int):
        """è®°å½•LLMè°ƒç”¨"""
        self.current_llm_calls += 1
        self.current_tokens += prompt_tokens + response_tokens
        
        self.execution_log.append({
            "timestamp": datetime.now().isoformat(),
            "call_number": self.current_llm_calls,
            "prompt_tokens": prompt_tokens,
            "response_tokens": response_tokens,
            "total_tokens": self.current_tokens
        })
    
    def get_remaining_resources(self) -> Dict[str, Any]:
        """è·å–å‰©ä½™èµ„æº"""
        elapsed_time = time.time() - self.start_time if self.start_time else 0
        
        return {
            "remaining_calls": self.max_llm_calls - self.current_llm_calls,
            "remaining_tokens": self.max_total_tokens - self.current_tokens,
            "remaining_time": self.max_execution_time - elapsed_time,
            "current_calls": self.current_llm_calls,
            "current_tokens": self.current_tokens,
            "elapsed_time": elapsed_time
        }
    
    def is_context_too_long(self, context: str) -> bool:
        """æ£€æŸ¥ä¸Šä¸‹æ–‡æ˜¯å¦è¿‡é•¿"""
        # ç®€å•çš„tokenä¼°ç®—ï¼šå­—ç¬¦æ•°/4
        estimated_tokens = len(context) / 4
        return estimated_tokens > self.max_context_length


class IntelligentToolOrchestrator:
    """
    æ™ºèƒ½å·¥å…·ç¼–æ’å™¨
    
    æ ¸å¿ƒåŠŸèƒ½:
    - LLMé©±åŠ¨çš„å·¥å…·é€‰æ‹©å’Œæ‰§è¡Œé¡ºåºå†³ç­–
    - ä¸Memoryç³»ç»Ÿé›†æˆï¼Œç»´æŠ¤å®Œæ•´å†å²
    - åŠ¨æ€è¯„ä¼°æ‰§è¡Œè¿›åº¦ï¼Œå†³å®šæ˜¯å¦ç»§ç»­
    - ç”Ÿæˆå®Œæ•´çš„é‡å­è®¡ç®—ä»£ç 
    """
    
    def __init__(self):
        """åˆå§‹åŒ–ç¼–æ’å™¨"""
        self.memory = Memory()
        self.available_tools = []  # å°†é€šè¿‡å·¥å…·æ³¨å†Œç³»ç»Ÿå¡«å……
        self.resource_manager = ResourceManager()
        self.execution_status = "ready"  # ready, running, completed, failed, stopped
        
    def register_tool(self, tool_class):
        """æ³¨å†Œå¯ç”¨å·¥å…·"""
        self.available_tools.append(tool_class)
        print(f"ğŸ”§ Registered tool: {tool_class.__name__}")
    
    def load_all_tools(self):
        """è‡ªåŠ¨åŠ è½½æ‰€æœ‰å¯ç”¨å·¥å…·"""
        try:
            # å¯¼å…¥å·¥å…·æ³¨å†Œç³»ç»Ÿ
            import sys
            import os
            
            # æ·»åŠ toolsç›®å½•åˆ°è·¯å¾„
            tools_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'tools')
            if tools_path not in sys.path:
                sys.path.append(tools_path)
            
            # å¯¼å…¥å¹¶ä½¿ç”¨å·¥å…·æ³¨å†Œç³»ç»Ÿ
            from registry import register_all_tools
            register_all_tools(self)
            
        except Exception as e:
            print(f"âš ï¸ Failed to auto-load tools: {e}")
            print("Please register tools manually using register_tool()")
    
    def get_execution_stats(self) -> Dict[str, Any]:
        """è·å–æ‰§è¡Œç»Ÿè®¡ä¿¡æ¯"""
        return {
            "status": self.execution_status,
            "available_tools": [tool.__name__ for tool in self.available_tools],
            "resource_stats": self.resource_manager.get_remaining_resources(),
            "execution_log": self.resource_manager.execution_log
        }
    
    def generate_quantum_code(self, user_query: str) -> Dict[str, Any]:
        """
        ä¸»å…¥å£ç‚¹ï¼šæ ¹æ®ç”¨æˆ·æŸ¥è¯¢ç”Ÿæˆå®Œæ•´çš„é‡å­ä»£ç 
        
        Args:
            user_query: ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€æŸ¥è¯¢
            
        Returns:
            åŒ…å«ä»£ç å’Œæ‰§è¡Œä¿¡æ¯çš„å­—å…¸
        """
        try:
            # å¼€å§‹æ‰§è¡Œ
            self.execution_status = "running"
            self.resource_manager.start_execution()
            self.memory.initialize_query(user_query)
            
            print(f"ğŸš€ Starting quantum code generation for: {user_query}")
            print(f"ğŸ“Š Available tools: {len(self.available_tools)}")
            
            # ä¸»æ‰§è¡Œå¾ªç¯
            max_iterations = 10  # é˜²æ­¢æ— é™å¾ªç¯
            iteration = 0
            stop_reason = "max_iterations"
            
            while iteration < max_iterations:
                iteration += 1
                print(f"\nğŸ”„ Iteration {iteration}")
                
                # æ£€æŸ¥èµ„æºé™åˆ¶
                if not self.resource_manager.can_make_llm_call():
                    print("â° Resource limits reached")
                    stop_reason = "resource_limit"
                    break
                
                # LLMé€‰æ‹©ä¸‹ä¸€ä¸ªå·¥å…·
                next_tool_name = self._llm_select_next_tool()
                print(f"ğŸ¯ LLM selected tool: {next_tool_name}")
                
                # æ£€æŸ¥æ˜¯å¦å®Œæˆ
                if next_tool_name == "COMPLETE":
                    stop_reason = "completed"
                    break
                
                # æ‰§è¡Œé€‰å®šçš„å·¥å…·
                tool_output = self._execute_tool(next_tool_name)
                
                # æ£€æŸ¥å·¥å…·æ‰§è¡Œæ˜¯å¦æˆåŠŸ
                if "error" in tool_output:
                    print(f"âŒ Tool execution failed: {tool_output['error']}")
                    # ç»§ç»­æ‰§è¡Œå…¶ä»–å·¥å…·ï¼Œä¸ä¸­æ–­æµç¨‹
                
                # å°†è¾“å‡ºæ·»åŠ åˆ°Memory
                self.memory.add_tool_output(next_tool_name, tool_output)
                
                # æ£€æŸ¥èµ„æºé™åˆ¶åå†æ¬¡è°ƒç”¨LLM
                if not self.resource_manager.can_make_llm_call():
                    print("â° Resource limits reached before progress evaluation")
                    stop_reason = "resource_limit"
                    break
                
                # LLMè¯„ä¼°æ˜¯å¦ç»§ç»­
                should_continue = self._llm_evaluate_progress()
                print(f"ğŸ¤” LLM decision: {'Continue' if should_continue else 'Stop'}")
                
                if not should_continue:
                    stop_reason = "llm_decision"
                    break
            
            self.execution_status = "completed"
            
            # è¿”å›å®Œæ•´ç»“æœ
            result = {
                "code": self.memory.get_final_code(),
                "stop_reason": stop_reason,
                "iterations": iteration,
                "resources_used": self.resource_manager.get_remaining_resources(),
                "memory_summary": self.memory.get_summary(),
                "execution_log": self.resource_manager.execution_log
            }
            
            print(f"\nâœ… Execution completed: {stop_reason}")
            print(f"ğŸ“Š Used {self.resource_manager.current_llm_calls} LLM calls")
            
            return result
            
        except Exception as e:
            self.execution_status = "failed"
            print(f"ğŸ’¥ Execution failed: {str(e)}")
            
            return {
                "code": self.memory.get_final_code() if self.memory.execution_history else "",
                "stop_reason": "error",
                "error": str(e),
                "iterations": iteration if 'iteration' in locals() else 0,
                "resources_used": self.resource_manager.get_remaining_resources(),
                "memory_summary": self.memory.get_summary() if self.memory.execution_history else "No execution history"
            }
    
    def _llm_select_next_tool(self) -> str:
        """
        ä½¿ç”¨LLMåŠ¨æ€é€‰æ‹©ä¸‹ä¸€ä¸ªè¦æ‰§è¡Œçš„å·¥å…·
        
        Returns:
            å·¥å…·åç§°æˆ–"COMPLETE"è¡¨ç¤ºå®Œæˆ
        """
        # è·å–ä¸Šä¸‹æ–‡å¹¶æ£€æŸ¥é•¿åº¦
        context = self.memory.get_context_for_tool("tool_selector")
        
        # å¦‚æœä¸Šä¸‹æ–‡è¿‡é•¿ï¼Œè¿›è¡Œå‹ç¼©
        if self.resource_manager.is_context_too_long(context):
            print("âš ï¸ Context too long, compressing...")
            # ç®€å•å‹ç¼©ï¼šåªä¿ç•™æœ€åå‡ æ­¥å’ŒåŸå§‹æŸ¥è¯¢
            lines = context.split('\n')
            compressed_context = '\n'.join(lines[:5] + lines[-10:])  # ä¿ç•™å‰5è¡Œå’Œå10è¡Œ
            context = compressed_context
        
        available_tools_str = ", ".join([tool.__name__ for tool in self.available_tools])
        
        prompt = f"""
Based on the following context, select the next tool to execute for quantum code generation.

{context}

Available tools: {available_tools_str}

Current task: Generate complete quantum code for the user's query.

Please respond with ONLY the tool name (e.g., "TFIMModelGenerator") or "COMPLETE" if all necessary steps are done.

Tool selection:"""

        # ä¼°ç®—tokenå¹¶è®°å½•
        estimated_prompt_tokens = len(prompt) // 4
        response = call_llm(prompt, temperature=0.1)
        estimated_response_tokens = len(response) // 4
        
        # è®°å½•åˆ°èµ„æºç®¡ç†å™¨
        self.resource_manager.record_llm_call(estimated_prompt_tokens, estimated_response_tokens)
        
        # æ¸…ç†å“åº”ï¼Œåªä¿ç•™å·¥å…·åç§°
        tool_name = response.strip().replace('"', '').replace("'", '')
        
        return tool_name
    
    def _llm_evaluate_progress(self) -> bool:
        """
        ä½¿ç”¨LLMè¯„ä¼°å½“å‰è¿›åº¦ï¼Œå†³å®šæ˜¯å¦ç»§ç»­æ‰§è¡Œ
        
        Returns:
            Trueè¡¨ç¤ºç»§ç»­ï¼ŒFalseè¡¨ç¤ºåœæ­¢
        """
        execution_summary = self.memory.get_execution_summary()
        resources = self.resource_manager.get_remaining_resources()
        
        prompt = f"""
Evaluate the current progress of quantum code generation:

Original Query: {execution_summary['original_query']}
Steps Executed: {execution_summary['total_steps']}
Tools Used: {execution_summary['executed_tools']}
Has Code: {execution_summary['has_code_fragments']}

Remaining Resources:
- LLM Calls: {resources['remaining_calls']}
- Tokens: {resources['remaining_tokens']}
- Time: {resources['remaining_time']:.1f}s

Analysis Guidelines:
- TFIM queries typically need: 1) Model parameters AND 2) Hamiltonian construction
- "Complete TFIM Hamiltonian" requires both model generation and Pauli operator construction
- "Hamiltonian" or "Pauli" in query suggests need for TFIMHamiltonianBuilder
- "VQE", "ground state", "energy", "optimization" in query requires full VQE pipeline:
  Model â†’ Hamiltonian â†’ Circuit â†’ Optimizer â†’ Executor â†’ Assembler
- "Calculate ground state energy" requires ALL 6 tools for complete code generation
- "Execute VQE", "run VQE", "compute energy" requires final QiskitTFIMExecutor and QiskitCodeAssembler
- "Complete code", "final code", "save file" requires QiskitCodeAssembler for integration
- Continue if query asks for complete/full implementation but missing components
- Continue if ground state energy calculation requested but no code assembler used yet
- Continue if VQE execution requested but no final code integration
- Stop only if all requested components including final code assembly are implemented

Based on this analysis and remaining resources, should we continue with more tools or is the task complete?

Respond with only "CONTINUE" or "STOP".

Decision:"""

        # ä¼°ç®—tokenå¹¶è®°å½•
        estimated_prompt_tokens = len(prompt) // 4
        response = call_llm(prompt, temperature=0.1)
        estimated_response_tokens = len(response) // 4
        
        # è®°å½•åˆ°èµ„æºç®¡ç†å™¨
        self.resource_manager.record_llm_call(estimated_prompt_tokens, estimated_response_tokens)
        
        return response.strip().upper() == "CONTINUE"
    
    def _execute_tool(self, tool_name: str) -> Dict[str, Any]:
        """
        æ‰§è¡ŒæŒ‡å®šçš„å·¥å…·
        
        Args:
            tool_name: å·¥å…·åç§°
            
        Returns:
            å·¥å…·æ‰§è¡Œç»“æœ
        """
        # æŸ¥æ‰¾å·¥å…·ç±»
        tool_class = None
        for tool in self.available_tools:
            if tool.__name__ == tool_name:
                tool_class = tool
                break
        
        if tool_class is None:
            return {
                "error": f"Tool {tool_name} not found",
                "code": "",
                "notes": f"Failed to find tool: {tool_name}"
            }
        
        try:
            # å®ä¾‹åŒ–å·¥å…·
            tool_instance = tool_class()
            
            # è·å–å½“å‰ä¸Šä¸‹æ–‡
            context = self.memory.get_context_for_tool(tool_name)
            
            # æ‰§è¡Œå·¥å…·
            result = tool_instance.execute(context)
            
            return result
            
        except Exception as e:
            return {
                "error": str(e),
                "code": "",
                "notes": f"Tool execution failed: {str(e)}"
            }
    
    def get_memory_summary(self) -> str:
        """è·å–Memoryç³»ç»Ÿæ‘˜è¦"""
        return self.memory.get_summary()
    
    def get_execution_history(self) -> List[Dict[str, Any]]:
        """è·å–å®Œæ•´æ‰§è¡Œå†å²"""
        return self.memory.execution_history




# ç¤ºä¾‹ä½¿ç”¨
if __name__ == "__main__":
    print("IntelligentToolOrchestrator is ready.")
    print("Please use this class by importing it and registering real tools.")
    print("Example:")
    print("  orchestrator = IntelligentToolOrchestrator()")
    print("  orchestrator.load_all_tools()  # Auto-load tools")
    print("  result = orchestrator.generate_quantum_code('Your query')")