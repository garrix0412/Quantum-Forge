"""
QML Training Engine - QuantumForge V5

QML训练参数标准化器，接收来自parameter_matcher的参数，进行QML训练特定的验证、标准化和默认值处理。
遵循QuantumForge V5的LLM驱动架构：信任上游parameter_matcher分析，专注训练逻辑。
保持简洁，遵循原始qml.py的训练模式。
"""

from typing import Dict, Any

# 导入基类
try:
    from ..base_component import BaseComponent
except ImportError:
    import sys
    import os
    sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
    from components.base_component import BaseComponent


class QMLTrainingEngine(BaseComponent):
    """QML训练引擎 - 信任parameter_matcher的智能参数分析，专注训练逻辑"""
    
    description = "Build training loops for QML hybrid models. Supports Adam, SGD, RMSprop optimizers with configurable learning rates and epochs. Handles binary classification training with NLLLoss. Simple training progress tracking. Trusts parameter_matcher for training configuration."
    
    def execute(self, query: str, params: Dict[str, Any]) -> Dict[str, Any]:
        """构建QML训练代码"""
        # 信任parameter_matcher提供的参数
        training_params = params.copy()
        
        # 应用QML训练特定的默认值
        complete_params = self._apply_qml_training_defaults(training_params)
        
        # 参数获取
        optimizer_type = complete_params.get("optimizer", "Adam")
        learning_rate = complete_params.get("learning_rate", 0.001)
        epochs = complete_params.get("epochs", 10)
        loss_function = complete_params.get("loss_function", "NLLLoss")
        
        # 生成训练代码
        code = self._generate_training_code(complete_params)
        
        # 计算training_info
        training_info = self._calculate_training_info(complete_params)
        
        # 简要描述
        notes = f"QML training: {optimizer_type}(lr={learning_rate}) + {loss_function}, {epochs} epochs"
        
        return {
            "code": code,
            "notes": notes,
            "training_info": training_info
        }
    
    def _apply_qml_training_defaults(self, params: Dict[str, Any]) -> Dict[str, Any]:
        """应用QML训练特定的默认值 - 信任parameter_matcher"""
        # 设置QML训练默认值（基于原始qml.py）
        defaults = {
            "optimizer": "Adam",               # Adam优化器（与原代码一致）
            "learning_rate": 0.001,            # 学习率（与原代码一致）
            "epochs": 10,                      # 训练轮数（与原代码一致）
            "loss_function": "NLLLoss",        # 损失函数（与原代码一致）
            "batch_size": 1,                   # 批次大小（与原代码一致）
            "print_progress": True,            # 打印训练进度
            "store_loss_history": True,        # 存储损失历史
            "model_eval_mode": True,           # 训练后设置eval模式
            "gradient_clipping": None          # 不使用梯度裁剪
        }
        
        # 合并参数，保持parameter_matcher提供的参数优先
        complete_params = {**defaults, **params}
        
        # 智能调整：基于数据集规模调整训练参数
        dataset_info = params.get("dataset_info", {})
        if dataset_info:
            total_samples = dataset_info.get("total_train_samples", 200)
            
            # 基于数据集大小调整epochs
            if "epochs" not in params:
                if total_samples <= 200:
                    complete_params["epochs"] = 5    # 小数据集减少epoch避免过拟合
                elif total_samples > 1000:
                    complete_params["epochs"] = 15   # 大数据集增加epoch充分训练
        
        # 网络复杂度调整
        network_info = params.get("network_info", {})
        if network_info:
            total_params = network_info.get("total_parameters", 5000)
            
            # 基于网络参数数量调整学习率
            if "learning_rate" not in params:
                if total_params < 3000:
                    complete_params["learning_rate"] = 0.005   # 小网络提高学习率
                elif total_params > 10000:
                    complete_params["learning_rate"] = 0.0005  # 大网络降低学习率
        
        return complete_params
    
    def _generate_training_code(self, params: Dict[str, Any]) -> str:
        """生成训练代码"""
        optimizer_type = params["optimizer"]
        learning_rate = params["learning_rate"]
        epochs = params["epochs"]
        loss_function = params["loss_function"]
        batch_size = params["batch_size"]
        print_progress = params["print_progress"]
        store_loss_history = params["store_loss_history"]
        model_eval_mode = params["model_eval_mode"]
        
        # 生成完整的训练代码
        code = f'''# QML Training Engine - Generated by QuantumForge V5
import torch
import torch.optim as optim
from torch.nn import NLLLoss, CrossEntropyLoss, BCELoss
from torch import no_grad

# Training Configuration
OPTIMIZER_TYPE = "{optimizer_type}"
LEARNING_RATE = {learning_rate}
EPOCHS = {epochs}
LOSS_FUNCTION = "{loss_function}"
BATCH_SIZE = {batch_size}
PRINT_PROGRESS = {str(print_progress).lower()}
STORE_LOSS_HISTORY = {str(store_loss_history).lower()}

print(f"QML Training Setup: {{OPTIMIZER_TYPE}}(lr={{LEARNING_RATE}}) + {{LOSS_FUNCTION}}, {{EPOCHS}} epochs")

def setup_training(model, train_loader):
    """
    Setup QML training components
    
    Args:
        model: QML hybrid network model
        train_loader: Training data loader
        
    Returns:
        tuple: (optimizer, loss_function, training_config)
    """
    # Setup optimizer'''
        
        # 添加优化器配置代码
        code += self._generate_optimizer_code(optimizer_type, learning_rate)
        
        # 添加损失函数配置代码
        code += self._generate_loss_function_code(loss_function)
        
        # 添加主训练循环代码
        code += f'''
    
    training_config = {{
        "epochs": EPOCHS,
        "batch_size": BATCH_SIZE,
        "print_progress": PRINT_PROGRESS,
        "store_loss_history": STORE_LOSS_HISTORY
    }}
    
    return optimizer, loss_func, training_config

def train_qml_model(model, train_loader, optimizer, loss_func, training_config):
    """
    Train QML hybrid model (following original qml.py pattern)
    
    Args:
        model: QML hybrid network
        train_loader: Training data loader
        optimizer: Configured optimizer
        loss_func: Loss function
        training_config: Training configuration dict
        
    Returns:
        list: Loss history if store_loss_history=True, else empty list
    """
    epochs = training_config["epochs"]
    print_progress = training_config["print_progress"]
    store_loss_history = training_config["store_loss_history"]
    
    loss_list = [] if store_loss_history else None
    model.train()  # Set model to training mode
    
    print("Starting QML training...")
    
    for epoch in range(epochs):
        total_loss = []
        for batch_idx, (data, target) in enumerate(train_loader):
            optimizer.zero_grad(set_to_none=True)  # Initialize gradient (memory efficient)
            output = model(data)  # Forward pass
            loss = loss_func(output, target)  # Calculate loss
            loss.backward()  # Backward pass
            optimizer.step()  # Optimize weights
            total_loss.append(loss.item())  # Store loss
        
        epoch_loss = sum(total_loss) / len(total_loss)
        if store_loss_history:
            loss_list.append(epoch_loss)
        
        if print_progress:
            progress_pct = 100.0 * (epoch + 1) / epochs
            print(f"Training [{{progress_pct:.0f}}%]\\tLoss: {{epoch_loss:.4f}}")
    
    print("QML training completed!")
    return loss_list if store_loss_history else []

def evaluate_qml_model(model, test_loader, loss_func):
    """
    Evaluate QML model performance (following original qml.py pattern)
    
    Args:
        model: Trained QML hybrid network
        test_loader: Test data loader  
        loss_func: Loss function
        
    Returns:
        dict: Evaluation results with accuracy and loss
    """
    model.eval()  # Set model to evaluation mode
    total_loss = []
    correct = 0
    
    print("Evaluating QML model...")
    
    with no_grad():
        for batch_idx, (data, target) in enumerate(test_loader):
            output = model(data)
            
            # Handle dimensionality (following original qml.py pattern)
            if len(output.shape) == 1:
                output = output.reshape(1, *output.shape)
            
            pred = output.argmax(dim=1, keepdim=True)
            correct += pred.eq(target.view_as(pred)).sum().item()
            
            loss = loss_func(output, target)
            total_loss.append(loss.item())
    
    # Calculate final metrics
    test_loss = sum(total_loss) / len(total_loss)
    accuracy = correct / len(test_loader) / test_loader.batch_size * 100
    
    results = {{
        "test_loss": test_loss,
        "accuracy": accuracy,
        "correct_predictions": correct,
        "total_samples": len(test_loader) * test_loader.batch_size
    }}
    
    print(f"Performance on test data:")
    print(f"\\tLoss: {{test_loss:.4f}}")
    print(f"\\tAccuracy: {{accuracy:.1f}}%")
    
    return results

# Example usage:
# optimizer, loss_func, config = setup_training(model, train_loader)
# loss_history = train_qml_model(model, train_loader, optimizer, loss_func, config)
# eval_results = evaluate_qml_model(model, test_loader, loss_func)
print("QML training engine ready for hybrid quantum-classical model training")
'''
        
        return code
    
    def _generate_optimizer_code(self, optimizer_type: str, learning_rate: float) -> str:
        """生成优化器配置代码"""
        if optimizer_type == "Adam":
            return f'''
    optimizer = optim.Adam(model.parameters(), lr={learning_rate})'''
        elif optimizer_type == "SGD":
            return f'''
    optimizer = optim.SGD(model.parameters(), lr={learning_rate}, momentum=0.9)'''
        elif optimizer_type == "RMSprop":
            return f'''
    optimizer = optim.RMSprop(model.parameters(), lr={learning_rate})'''
        else:
            # 默认使用Adam
            return f'''
    optimizer = optim.Adam(model.parameters(), lr={learning_rate})  # Default to Adam'''
    
    def _generate_loss_function_code(self, loss_function: str) -> str:
        """生成损失函数配置代码"""
        if loss_function == "NLLLoss":
            return f'''
    
    # Setup loss function (NLLLoss for binary classification)
    loss_func = NLLLoss()'''
        elif loss_function == "CrossEntropyLoss":
            return f'''
    
    # Setup loss function (CrossEntropyLoss)
    loss_func = CrossEntropyLoss()'''
        elif loss_function == "BCELoss":
            return f'''
    
    # Setup loss function (BCELoss for binary classification)
    loss_func = BCELoss()'''
        else:
            # 默认使用NLLLoss
            return f'''
    
    # Setup loss function (NLLLoss - default for QML binary classification)
    loss_func = NLLLoss()'''
    
    def _calculate_training_info(self, params: Dict[str, Any]) -> Dict[str, Any]:
        """计算训练配置信息"""
        optimizer_type = params["optimizer"]
        learning_rate = params["learning_rate"]
        epochs = params["epochs"]
        loss_function = params["loss_function"]
        batch_size = params["batch_size"]
        
        # 估算训练时间和资源需求
        dataset_info = params.get("dataset_info", {})
        network_info = params.get("network_info", {})
        
        if dataset_info and network_info:
            total_samples = dataset_info.get("total_train_samples", 200)
            total_params = network_info.get("total_parameters", 5000)
            
            # 估算每个epoch的迭代次数
            iterations_per_epoch = total_samples // batch_size
            total_iterations = iterations_per_epoch * epochs
            
            # 简单的时间估算（基于QML特点调整）
            estimated_time_minutes = (total_iterations / 100) + (total_params / 10000)  # QML训练相对较快
        else:
            iterations_per_epoch = 200  # 默认估算
            total_iterations = iterations_per_epoch * epochs
            estimated_time_minutes = 2  # 默认估算
        
        return {
            "optimizer_type": optimizer_type,
            "learning_rate": learning_rate,
            "epochs": epochs,
            "loss_function": loss_function,
            "batch_size": batch_size,
            "iterations_per_epoch": iterations_per_epoch,
            "total_iterations": total_iterations,
            "estimated_time_minutes": max(1, int(estimated_time_minutes)),
            "training_features": {
                "progress_tracking": params["print_progress"],
                "loss_history": params["store_loss_history"],
                "model_evaluation": params["model_eval_mode"]
            }
        }