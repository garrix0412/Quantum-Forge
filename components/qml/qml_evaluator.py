"""
QML Evaluator - QuantumForge V5

QML评估参数标准化器，接收来自parameter_matcher的参数，进行QML模型评估特定的验证、标准化和默认值处理。
遵循QuantumForge V5的LLM驱动架构：信任上游parameter_matcher分析，专注评估逻辑。
遵循原始qml.py的简洁评估模式。
"""

from typing import Dict, Any

# 导入基类
try:
    from ..base_component import BaseComponent
except ImportError:
    import sys
    import os
    sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
    from components.base_component import BaseComponent


class QMLEvaluator(BaseComponent):
    """QML评估器 - 信任parameter_matcher的智能参数分析，专注模型评估"""
    
    description = "Build evaluation and testing code for trained QML models. Supports accuracy calculation, confusion matrix, prediction analysis for binary classification. Handles model performance metrics and result visualization. Follows original qml.py evaluation pattern. Trusts parameter_matcher for evaluation configuration."
    
    def execute(self, query: str, params: Dict[str, Any]) -> Dict[str, Any]:
        """构建QML评估代码"""
        # 信任parameter_matcher提供的参数
        eval_params = params.copy()
        
        # 应用QML评估特定的默认值
        complete_params = self._apply_qml_evaluation_defaults(eval_params)
        
        # 参数获取
        metrics = complete_params.get("metrics", ["accuracy", "loss"])
        include_confusion_matrix = complete_params.get("include_confusion_matrix", True)
        include_predictions = complete_params.get("include_predictions", False)
        save_results = complete_params.get("save_results", False)
        
        # 生成评估代码
        code = self._generate_evaluation_code(complete_params)
        
        # 计算evaluation_info
        evaluation_info = self._calculate_evaluation_info(complete_params)
        
        # 简要描述
        metrics_str = "+".join(metrics)
        notes = f"QML evaluation: {metrics_str}, binary classification analysis"
        
        return {
            "code": code,
            "notes": notes,
            "evaluation_info": evaluation_info
        }
    
    def _apply_qml_evaluation_defaults(self, params: Dict[str, Any]) -> Dict[str, Any]:
        """应用QML评估特定的默认值 - 信任parameter_matcher"""
        # 设置QML评估默认值（基于原始qml.py评估部分）
        defaults = {
            "metrics": ["accuracy", "loss"],           # 评估指标（与原代码一致）
            "include_confusion_matrix": True,          # 包含混淆矩阵
            "include_predictions": False,              # 是否包含详细预测结果
            "save_results": False,                     # 是否保存结果到文件
            "print_results": True,                     # 打印评估结果（与原代码一致）
            "detailed_analysis": False,                # 详细分析（类别级别的指标）
            "batch_evaluation": True,                  # 批量评估模式
            "return_predictions": False                # 是否返回预测结果
        }
        
        # 合并参数，保持parameter_matcher提供的参数优先
        complete_params = {**defaults, **params}
        
        # 智能调整：基于数据集大小调整评估配置
        dataset_info = params.get("dataset_info", {})
        if dataset_info:
            total_test_samples = dataset_info.get("total_test_samples", 100)
            
            # 小数据集启用详细分析
            if total_test_samples <= 100 and "detailed_analysis" not in params:
                complete_params["detailed_analysis"] = True
                complete_params["include_predictions"] = True
            
            # 大数据集简化输出
            if total_test_samples > 500 and "include_predictions" not in params:
                complete_params["include_predictions"] = False
        
        # 训练信息智能调整
        training_info = params.get("training_info", {})
        if training_info:
            # 如果训练时存储了损失历史，则包含损失对比
            if training_info.get("training_features", {}).get("loss_history", False):
                if "metrics" not in params and "loss" not in complete_params["metrics"]:
                    complete_params["metrics"].append("loss")
        
        return complete_params
    
    def _generate_evaluation_code(self, params: Dict[str, Any]) -> str:
        """生成评估代码"""
        metrics = params["metrics"]
        include_confusion_matrix = params["include_confusion_matrix"]
        include_predictions = params["include_predictions"]
        save_results = params["save_results"]
        print_results = params["print_results"]
        detailed_analysis = params["detailed_analysis"]
        return_predictions = params["return_predictions"]
        
        # 生成完整的评估代码
        code = f'''# QML Evaluator - Generated by QuantumForge V5
import torch
from torch import no_grad
import numpy as np

# Evaluation Configuration
METRICS = {metrics}
INCLUDE_CONFUSION_MATRIX = {str(include_confusion_matrix).lower()}
INCLUDE_PREDICTIONS = {str(include_predictions).lower()}
SAVE_RESULTS = {str(save_results).lower()}
PRINT_RESULTS = {str(print_results).lower()}
DETAILED_ANALYSIS = {str(detailed_analysis).lower()}
RETURN_PREDICTIONS = {str(return_predictions).lower()}

print(f"QML Evaluation Setup: {{\", \".join(METRICS)}} metrics for binary classification")

def evaluate_qml_model(model, test_loader, loss_func=None):
    """
    Comprehensive QML model evaluation (following original qml.py pattern)
    
    Args:
        model: Trained QML hybrid network
        test_loader: Test data loader
        loss_func: Loss function (optional, required for loss metric)
        
    Returns:
        dict: Comprehensive evaluation results
    """
    model.eval()  # Set model to evaluation mode
    print("Starting QML model evaluation...")
    
    # Initialize metrics tracking
    total_loss = [] if "loss" in METRICS and loss_func is not None else None
    correct_predictions = 0
    total_samples = 0
    
    # For detailed analysis
    all_predictions = [] if RETURN_PREDICTIONS or INCLUDE_PREDICTIONS else None
    all_targets = [] if INCLUDE_CONFUSION_MATRIX or DETAILED_ANALYSIS else None
    all_outputs = [] if DETAILED_ANALYSIS else None
    
    with no_grad():
        for batch_idx, (data, target) in enumerate(test_loader):
            output = model(data)
            
            # Handle dimensionality (following original qml.py pattern)
            if len(output.shape) == 1:
                output = output.reshape(1, *output.shape)
            
            # Get predictions
            pred = output.argmax(dim=1, keepdim=True)
            correct_predictions += pred.eq(target.view_as(pred)).sum().item()
            total_samples += target.size(0)
            
            # Calculate loss if needed
            if total_loss is not None:
                loss = loss_func(output, target)
                total_loss.append(loss.item())
            
            # Store detailed results if needed
            if all_predictions is not None:
                all_predictions.extend(pred.cpu().numpy().flatten())
            if all_targets is not None:
                all_targets.extend(target.cpu().numpy())
            if all_outputs is not None:
                all_outputs.extend(output.cpu().numpy())
    
    # Calculate basic metrics
    accuracy = (correct_predictions / total_samples) * 100
    test_loss = sum(total_loss) / len(total_loss) if total_loss else None
    
    # Prepare results dictionary
    results = {{
        "accuracy": accuracy,
        "correct_predictions": correct_predictions,
        "total_samples": total_samples,
        "error_rate": 100 - accuracy
    }}
    
    if test_loss is not None:
        results["test_loss"] = test_loss
    
    # Add confusion matrix if requested
    if INCLUDE_CONFUSION_MATRIX and all_targets is not None and all_predictions is not None:
        results["confusion_matrix"] = calculate_confusion_matrix(all_targets, all_predictions)
    
    # Add detailed predictions if requested  
    if INCLUDE_PREDICTIONS and all_predictions is not None:
        results["predictions"] = all_predictions[:50]  # Limit to first 50 for display
        results["targets"] = all_targets[:50] if all_targets is not None else None
    
    # Add detailed analysis if requested
    if DETAILED_ANALYSIS and all_targets is not None and all_predictions is not None:
        results["detailed_metrics"] = calculate_detailed_metrics(all_targets, all_predictions)
    
    # Print results (following original qml.py format)
    if PRINT_RESULTS:
        print_evaluation_results(results)
    
    # Save results if requested
    if SAVE_RESULTS:
        save_evaluation_results(results)
    
    return results

def calculate_confusion_matrix(targets, predictions):
    """
    Calculate confusion matrix for binary classification
    
    Args:
        targets: True labels
        predictions: Predicted labels
        
    Returns:
        dict: Confusion matrix components
    """
    targets = np.array(targets)
    predictions = np.array(predictions)
    
    # For binary classification (0, 1)
    tp = np.sum((targets == 1) & (predictions == 1))  # True Positives
    tn = np.sum((targets == 0) & (predictions == 0))  # True Negatives  
    fp = np.sum((targets == 0) & (predictions == 1))  # False Positives
    fn = np.sum((targets == 1) & (predictions == 0))  # False Negatives
    
    return {{
        "true_positives": int(tp),
        "true_negatives": int(tn),
        "false_positives": int(fp),
        "false_negatives": int(fn),
        "matrix": [[int(tn), int(fp)], [int(fn), int(tp)]]  # [[TN, FP], [FN, TP]]
    }}

def calculate_detailed_metrics(targets, predictions):
    """
    Calculate detailed classification metrics
    
    Args:
        targets: True labels
        predictions: Predicted labels
        
    Returns:
        dict: Detailed metrics (precision, recall, f1-score)
    """
    targets = np.array(targets)
    predictions = np.array(predictions)
    
    # Calculate confusion matrix components
    tp = np.sum((targets == 1) & (predictions == 1))
    tn = np.sum((targets == 0) & (predictions == 0))
    fp = np.sum((targets == 0) & (predictions == 1))
    fn = np.sum((targets == 1) & (predictions == 0))
    
    # Calculate metrics with zero division protection
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0
    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0
    
    return {{
        "precision": precision,
        "recall": recall,
        "f1_score": f1_score,
        "specificity": specificity,
        "sensitivity": recall  # Same as recall
    }}

def print_evaluation_results(results):
    """
    Print evaluation results (following original qml.py format)
    
    Args:
        results: Evaluation results dictionary
    """
    print("\\n" + "="*50)
    print("QML Model Evaluation Results")
    print("="*50)
    
    # Basic metrics (following original qml.py format)
    if "test_loss" in results:
        print(f"Test Loss: {{results['test_loss']:.4f}}")
    print(f"Accuracy: {{results['accuracy']:.1f}}%")
    print(f"Correct Predictions: {{results['correct_predictions']}}/{{results['total_samples']}}")
    print(f"Error Rate: {{results['error_rate']:.1f}}%")
    
    # Confusion matrix
    if "confusion_matrix" in results:
        cm = results["confusion_matrix"]
        print(f"\\nConfusion Matrix:")
        print(f"  [[{{cm['matrix'][0][0]:3d}}, {{cm['matrix'][0][1]:3d}}]]  [[TN, FP]]")
        print(f"  [[{{cm['matrix'][1][0]:3d}}, {{cm['matrix'][1][1]:3d}}]]  [[FN, TP]]")
    
    # Detailed metrics
    if "detailed_metrics" in results:
        dm = results["detailed_metrics"]
        print(f"\\nDetailed Metrics:")
        print(f"  Precision: {{dm['precision']:.3f}}")
        print(f"  Recall: {{dm['recall']:.3f}}")
        print(f"  F1-Score: {{dm['f1_score']:.3f}}")
        print(f"  Specificity: {{dm['specificity']:.3f}}")
    
    print("="*50)

def save_evaluation_results(results, filename="qml_evaluation_results.txt"):
    """
    Save evaluation results to file
    
    Args:
        results: Evaluation results dictionary
        filename: Output filename
    """
    try:
        with open(filename, 'w') as f:
            f.write("QML Model Evaluation Results\\n")
            f.write("="*50 + "\\n")
            
            if "test_loss" in results:
                f.write(f"Test Loss: {{results['test_loss']:.4f}}\\n")
            f.write(f"Accuracy: {{results['accuracy']:.1f}}%\\n")
            f.write(f"Correct Predictions: {{results['correct_predictions']}}/{{results['total_samples']}}\\n")
            
            if "confusion_matrix" in results:
                cm = results["confusion_matrix"]
                f.write(f"\\nConfusion Matrix: {{cm['matrix']}}\\n")
            
            if "detailed_metrics" in results:
                dm = results["detailed_metrics"]
                f.write(f"\\nPrecision: {{dm['precision']:.3f}}\\n")
                f.write(f"Recall: {{dm['recall']:.3f}}\\n")
                f.write(f"F1-Score: {{dm['f1_score']:.3f}}\\n")
        
        print(f"Evaluation results saved to {{filename}}")
    except Exception as e:
        print(f"Error saving results: {{e}}")

def compare_with_classical(qml_results, classical_results=None):
    """
    Compare QML model with classical baseline (optional)
    
    Args:
        qml_results: QML evaluation results
        classical_results: Classical model results (optional)
        
    Returns:
        dict: Comparison results
    """
    if classical_results is None:
        return {{"comparison": "No classical baseline provided"}}
    
    accuracy_improvement = qml_results["accuracy"] - classical_results.get("accuracy", 0)
    
    comparison = {{
        "qml_accuracy": qml_results["accuracy"],
        "classical_accuracy": classical_results.get("accuracy", 0),
        "improvement": accuracy_improvement,
        "qml_advantage": accuracy_improvement > 0
    }}
    
    return comparison

# Example usage:
# results = evaluate_qml_model(model, test_loader, loss_func)
# comparison = compare_with_classical(results, classical_baseline)
print("QML evaluator ready for comprehensive model assessment")
'''
        
        return code
    
    def _calculate_evaluation_info(self, params: Dict[str, Any]) -> Dict[str, Any]:
        """计算评估配置信息"""
        metrics = params["metrics"]
        include_confusion_matrix = params["include_confusion_matrix"]
        include_predictions = params["include_predictions"]
        detailed_analysis = params["detailed_analysis"]
        
        # 估算评估复杂度和时间
        dataset_info = params.get("dataset_info", {})
        
        if dataset_info:
            total_test_samples = dataset_info.get("total_test_samples", 100)
            
            # 估算评估时间（基于样本数量和分析复杂度）
            base_time = total_test_samples / 1000  # 基础评估时间
            complexity_factor = 1.0
            
            if include_confusion_matrix:
                complexity_factor += 0.2
            if detailed_analysis:
                complexity_factor += 0.3
            if include_predictions:
                complexity_factor += 0.1
                
            estimated_eval_time = max(0.1, base_time * complexity_factor)
        else:
            total_test_samples = 100
            estimated_eval_time = 0.5
        
        return {
            "metrics": metrics,
            "analysis_features": {
                "confusion_matrix": include_confusion_matrix,
                "detailed_predictions": include_predictions,
                "detailed_metrics": detailed_analysis,
                "result_saving": params["save_results"]
            },
            "evaluation_scope": {
                "test_samples": total_test_samples,
                "classification_type": "binary",
                "output_classes": 2
            },
            "estimated_time_minutes": round(estimated_eval_time, 2),
            "complexity_level": "detailed" if detailed_analysis else "standard",
            "output_format": {
                "console_display": params["print_results"],
                "return_predictions": params["return_predictions"],
                "file_export": params["save_results"]
            }
        }